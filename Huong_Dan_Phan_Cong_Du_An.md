# üìã H∆Ø·ªöNG D·∫™N PH√ÇN C√îNG D·ª∞ √ÅN - PH√ÇN LO·∫†I TIN NH·∫ÆN R√ÅC

## üìä T·ªîNG QUAN D·ª∞ √ÅN

**T√™n d·ª± √°n:** H·ªá th·ªëng ph√¢n lo·∫°i tin nh·∫Øn r√°c s·ª≠ d·ª•ng Machine Learning  
**C√¥ng ngh·ªá:** Python, Streamlit, FastAPI, Scikit-learn  
**M·ª•c ti√™u:** X√¢y d·ª±ng h·ªá th·ªëng ho√†n ch·ªânh t·ª´ ti·ªÅn x·ª≠ l√Ω ƒë·∫øn tri·ªÉn khai API  

---

# üë§ PH·∫¶N 1: TI·ªÄN X·ª¨ L√ù VƒÇN B·∫¢N (ƒêo√†n Duy M·∫°nh)

## üéØ NHI·ªÜM V·ª§ CH√çNH
- Ph√°t tri·ªÉn module `text_preprocessor.py`
- X√¢y d·ª±ng pipeline ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n ƒëa ng√¥n ng·ªØ (Anh + Vi·ªát)
- T·∫°o b·ªô t·ª´ ƒëi·ªÉn stopwords v√† teencode cho c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát
- Vi·∫øt unit tests

## üìÅ FILE CH√çNH: `text_preprocessor.py`

### 1. IMPORT V√Ä THI·∫æT L·∫¨P C∆† B·∫¢N

```python
import re
import string
from sklearn.base import BaseEstimator, TransformerMixin
```

**Gi·∫£i th√≠ch:**
- `re`: Th∆∞ vi·ªán regex ƒë·ªÉ x·ª≠ l√Ω pattern matching
- `string`: Cung c·∫•p c√°c h·∫±ng s·ªë nh∆∞ punctuation
- `BaseEstimator, TransformerMixin`: ƒê·ªÉ t·∫°o custom transformer t∆∞∆°ng th√≠ch v·ªõi sklearn

### 2. B·ªò T·ª™ ƒêI·ªÇN STOPWORDS ƒêA NG√îN NG·ªÆ

```python
# Stopwords ti·∫øng Anh (s·ª≠ d·ª•ng NLTK ho·∫∑c t·ª± ƒë·ªãnh nghƒ©a)
EN_STOPWORDS = {
    "i", "me", "my", "myself", "we", "our", "ours", "ourselves", "you", "your", 
    "yours", "yourself", "yourselves", "he", "him", "his", "himself", "she", 
    "her", "hers", "herself", "it", "its", "itself", "they", "them", "their", 
    "theirs", "themselves", "what", "which", "who", "whom", "this", "that", 
    "these", "those", "am", "is", "are", "was", "were", "be", "been", "being", 
    "have", "has", "had", "having", "do", "does", "did", "doing", "a", "an", 
    "the", "and", "but", "if", "or", "because", "as", "until", "while", "of", 
    "at", "by", "for", "with", "through", "during", "before", "after", "above", 
    "below", "up", "down", "in", "out", "on", "off", "over", "under", "again", 
    "further", "then", "once"
}

# Stopwords ti·∫øng Vi·ªát (n·∫øu c·∫ßn x·ª≠ l√Ω d·ªØ li·ªáu ti·∫øng Vi·ªát)
VN_STOPWORDS = {
    "l√†","v√†","c·ªßa","c√≥","cho","nh√©","nh·ªâ","th√¨","l·∫°i","n√†y","kia","·∫•y",
    "ƒë√£","ƒëang","s·∫Ω","r·ªìi","v·ªõi","ƒë∆∞·ª£c","khi","ƒëi","ƒë·∫øn","t·ª´","trong",
    "ra","v√†o","c≈©ng","nh∆∞ng","n·∫øu","v√¨","do","tr√™n","d∆∞·ªõi","n·ªØa","n√™n"
}

# K·∫øt h·ª£p stopwords theo ng√¥n ng·ªØ
def get_stopwords(language="auto"):
    if language == "en":
        return EN_STOPWORDS
    elif language == "vi":
        return VN_STOPWORDS
    elif language == "auto":
        return EN_STOPWORDS.union(VN_STOPWORDS)  # K·∫øt h·ª£p c·∫£ 2
    else:
        return set()
```

**Gi·∫£i th√≠ch:**
- **EN_STOPWORDS**: T·ª´ d·ª´ng ti·∫øng Anh ph·ªï bi·∫øn (articles, pronouns, prepositions)
- **VN_STOPWORDS**: T·ª´ d·ª´ng ti·∫øng Vi·ªát (ch·ªâ c·∫ßn n·∫øu d·ªØ li·ªáu c√≥ ti·∫øng Vi·ªát)
- **Auto-detection**: T·ª± ƒë·ªông nh·∫≠n di·ªán ng√¥n ng·ªØ ho·∫∑c x·ª≠ l√Ω c·∫£ 2

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **T√≠ch h·ª£p NLTK**: `from nltk.corpus import stopwords` cho ti·∫øng Anh chu·∫©n
2. **Language detection**: Th√™m `langdetect` ƒë·ªÉ t·ª± ƒë·ªông nh·∫≠n di·ªán ng√¥n ng·ªØ
3. **Flexible stopwords**: Cho ph√©p ng∆∞·ªùi d√πng ch·ªçn ng√¥n ng·ªØ c·ª• th·ªÉ

### 3. B·ªò T·ª™ ƒêI·ªÇN CHU·∫®N H√ìA VƒÇN B·∫¢N

```python
# Internet slang v√† abbreviations ti·∫øng Anh (cho SMS/chat data)
EN_SLANG_MAP = {
    "u": "you", "ur": "your", "r": "are", "n": "and", "2": "to", "4": "for",
    "b4": "before", "c": "see", "omg": "oh my god", "lol": "laugh out loud", 
    "rofl": "rolling on floor laughing", "brb": "be right back", "ttyl": "talk to you later",
    "w8": "wait", "gr8": "great", "m8": "mate", "l8r": "later", "2morrow": "tomorrow",
    "2day": "today", "2nite": "tonight", "pls": "please", "plz": "please", 
    "thx": "thanks", "tx": "thanks", "tnx": "thanks", "msg": "message",
    "txt": "text", "pic": "picture", "vid": "video", "luv": "love", "wanna": "want to",
    "gonna": "going to", "dunno": "don't know", "kinda": "kind of", "sorta": "sort of"
}

# Teencode ti·∫øng Vi·ªát (ch·ªâ d√πng khi c√≥ d·ªØ li·ªáu ti·∫øng Vi·ªát)
VN_TEENCODE_MAP = {
    "ko": "kh√¥ng", "k": "kh√¥ng", "kh": "kh√¥ng",
    "dc": "ƒë∆∞·ª£c", "ƒëc": "ƒë∆∞·ª£c", 
    "mik": "m√¨nh", "mk": "m√¨nh",
    "bn": "b·∫°n", "ban": "b·∫°n",
    "j": "g√¨", "ji": "g√¨", "gi": "g√¨",
    "vs": "v·ªõi", "voi": "v·ªõi", "w": "v·ªõi"
}

def normalize_slang(text: str, language="en") -> str:
    """Chu·∫©n h√≥a slang/teencode theo ng√¥n ng·ªØ"""
    if language == "en":
        slang_map = EN_SLANG_MAP
    elif language == "vi":
        slang_map = VN_TEENCODE_MAP
    else:
        # Auto mode: th·ª≠ c·∫£ 2
        slang_map = {**EN_SLANG_MAP, **VN_TEENCODE_MAP}
    
    # Normalization logic here...
    return text
```

**Gi·∫£i th√≠ch:**
- **EN_SLANG_MAP**: Internet slang ti·∫øng Anh ph·ªï bi·∫øn trong SMS/chat
- **VN_TEENCODE_MAP**: Teencode ti·∫øng Vi·ªát (ch·ªâ d√πng khi c·∫ßn)
- **Language-aware**: Ch·ªçn dictionary ph√π h·ª£p v·ªõi ng√¥n ng·ªØ d·ªØ li·ªáu

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **M·ªü r·ªông EN_SLANG**: Th√™m 200+ abbreviations ti·∫øng Anh
2. **Conditional loading**: Ch·ªâ load VN dictionary khi c·∫ßn
3. **Auto-detection**: T·ª± ƒë·ªông ch·ªçn dictionary d·ª±a tr√™n ng√¥n ng·ªØ text

### 4. REGEX PATTERNS

```python
EMOJI_PATTERN = re.compile(r"[\U00010000-\U0010ffff]", flags=re.UNICODE)
URL_PATTERN = re.compile(r"https?://\S+|www\.\S+", flags=re.IGNORECASE)
EMAIL_PATTERN = re.compile(r"[\w\.-]+@[\w\.-]+", flags=re.IGNORECASE)
PHONE_PATTERN = re.compile(r"\b(?:\+?\d[\d\- ]{7,}\d)\b")
MONEY_PATTERN = re.compile(r"\b\d+[.,]?\d*\s*(?:k|ƒë|vnd|vnƒë|usd|$)\b", flags=re.IGNORECASE)
```

**Gi·∫£i th√≠ch t·ª´ng pattern:**

- **EMOJI_PATTERN**: Nh·∫≠n di·ªán emoji Unicode
  - `[\U00010000-\U0010ffff]`: Kho·∫£ng Unicode ch·ª©a emoji
  - `re.UNICODE`: H·ªó tr·ª£ Unicode ƒë·∫ßy ƒë·ªß

- **URL_PATTERN**: Nh·∫≠n di·ªán URL
  - `https?://`: B·∫Øt ƒë·∫ßu v·ªõi http ho·∫∑c https
  - `\S+`: M·ªôt ho·∫∑c nhi·ªÅu k√Ω t·ª± kh√¥ng ph·∫£i space
  - `www\.\S+`: Ho·∫∑c b·∫Øt ƒë·∫ßu b·∫±ng www.

- **EMAIL_PATTERN**: Nh·∫≠n di·ªán email
  - `[\w\.-]+`: M·ªôt ho·∫∑c nhi·ªÅu ch·ªØ c√°i, s·ªë, d·∫•u ch·∫•m, g·∫°ch ngang
  - `@`: K√Ω t·ª± @
  - `[\w\.-]+`: Domain name

- **PHONE_PATTERN**: Nh·∫≠n di·ªán s·ªë ƒëi·ªán tho·∫°i
  - `\b`: Word boundary
  - `\+?`: D·∫•u + t√πy ch·ªçn (cho s·ªë qu·ªëc t·∫ø)
  - `\d[\d\- ]{7,}\d`: S·ªë ƒë·∫ßu, 7+ k√Ω t·ª± s·ªë/d·∫•u/space, s·ªë cu·ªëi

- **MONEY_PATTERN**: Nh·∫≠n di·ªán s·ªë ti·ªÅn
  - `\d+[.,]?\d*`: S·ªë v·ªõi d·∫•u ph·∫©y/ch·∫•m t√πy ch·ªçn
  - `(?:k|ƒë|vnd|vnƒë|usd|$)`: ƒê∆°n v·ªã ti·ªÅn t·ªá

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **T·ªëi ∆∞u regex:** C·∫£i thi·ªán ƒë·ªô ch√≠nh x√°c
2. **Th√™m patterns:** Hashtag, mention (@user), s·ªë th·∫ª t√≠n d·ª•ng
3. **Test regex:** Vi·∫øt unit tests cho t·ª´ng pattern

### 5. H√ÄM CHU·∫®N H√ìA TEENCODE

```python
def normalize_teencode(text: str) -> str:
    tokens = text.split()
    out = []
    for t in tokens:
        key = t.lower().strip()
        # Lo·∫°i b·ªè d·∫•u c√¢u ·ªü cu·ªëi t·ª´
        punct_end = ""
        while key and key[-1] in string.punctuation:
            punct_end = key[-1] + punct_end
            key = key[:-1]
        
        # T√¨m trong teencode map
        normalized = TEENCODE_MAP.get(key, t)
        out.append(normalized + punct_end)
    return " ".join(out)
```

**Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc:**
1. **`text.split()`**: T√°ch th√†nh c√°c t·ª´
2. **`t.lower().strip()`**: Chuy·ªÉn th∆∞·ªùng v√† lo·∫°i space
3. **X·ª≠ l√Ω d·∫•u c√¢u**: T√°ch d·∫•u c√¢u ra kh·ªèi t·ª´ ƒë·ªÉ mapping ch√≠nh x√°c
4. **`TEENCODE_MAP.get(key, t)`**: T√¨m trong dictionary, n·∫øu kh√¥ng c√≥ th√¨ gi·ªØ nguy√™n
5. **Gh√©p l·∫°i**: T·ª´ ƒë√£ chu·∫©n h√≥a + d·∫•u c√¢u ban ƒë·∫ßu

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **X·ª≠ l√Ω case ph·ª©c t·∫°p:** T·ª´ c√≥ nhi·ªÅu d·∫•u c√¢u
2. **Optimize performance:** S·ª≠ d·ª•ng comprehension ho·∫∑c vectorization
3. **Handle edge cases:** Empty string, special characters

### 6. H√ÄM TOKENIZATION ƒêA NG√îN NG·ªÆ

```python
def simple_tokenize(text: str, language="en") -> str:
    """Tokenization c∆° b·∫£n cho c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát"""
    
    # X·ª≠ l√Ω contractions ti·∫øng Anh
    if language in ["en", "auto"]:
        contractions = {
            "won't": "will not", "can't": "cannot", "n't": " not",
            "'re": " are", "'ve": " have", "'ll": " will", "'d": " would",
            "'m": " am", "'s": " is"  # ho·∫∑c "'s": " has" t√πy context
        }
        for contraction, expansion in contractions.items():
            text = text.replace(contraction, expansion)
    
    # Th√™m kho·∫£ng tr·∫Øng tr∆∞·ªõc/sau d·∫•u c√¢u
    text = re.sub(r'([.!?;:,])', r' \1 ', text)
    # Th√™m kho·∫£ng tr·∫Øng tr∆∞·ªõc/sau d·∫•u ngo·∫∑c
    text = re.sub(r'([()\[\]{}])', r' \1 ', text)
    # X·ª≠ l√Ω d·∫•u ngo·∫∑c k√©p
    text = re.sub(r'(["\'])', r' \1 ', text)
    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def advanced_tokenize(text: str, language="en") -> str:
    """Tokenization n√¢ng cao v·ªõi NLTK ho·∫∑c spaCy"""
    try:
        if language == "en":
            import nltk
            from nltk.tokenize import word_tokenize
            tokens = word_tokenize(text.lower())
            return " ".join(tokens)
        elif language == "vi":
            # C√≥ th·ªÉ d√πng underthesea cho ti·∫øng Vi·ªát
            # from underthesea import word_tokenize
            # tokens = word_tokenize(text)
            return simple_tokenize(text, "vi")
        else:
            return simple_tokenize(text, "auto")
    except ImportError:
        # Fallback v·ªÅ simple tokenization
        return simple_tokenize(text, language)
```

**Gi·∫£i th√≠ch c·∫£i ti·∫øn:**
- **Contractions handling**: X·ª≠ l√Ω "don't" ‚Üí "do not" cho ti·∫øng Anh
- **Language-specific**: Tokenization kh√°c nhau cho t·ª´ng ng√¥n ng·ªØ
- **Advanced options**: T√≠ch h·ª£p NLTK/spaCy khi c√≥ th·ªÉ
- **Fallback mechanism**: D√πng simple tokenization khi th∆∞ vi·ªán n√¢ng cao kh√¥ng c√≥

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **NLTK integration**: T√≠ch h·ª£p `nltk.tokenize` cho ti·∫øng Anh
2. **Language detection**: Auto-detect ng√¥n ng·ªØ ƒë·ªÉ ch·ªçn tokenizer
3. **Performance**: Benchmark c√°c ph∆∞∆°ng ph√°p tokenization kh√°c nhau

### 7. H√ÄM TI·ªÄN X·ª¨ L√ù CH√çNH

```python
def basic_preprocess(text: str,
        lowercase: bool = True,
        rm_punct: bool = True,
        rm_numbers: bool = False,
        rm_emoji: bool = True,
        use_slang_normalization: bool = True,
        rm_stopwords: bool = True,
        keep_placeholders: bool = True,
        use_tokenize: bool = True,
        language: str = "en") -> str:
```

**Gi·∫£i th√≠ch c√°c tham s·ªë:**
- **lowercase**: Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
- **rm_punct**: Lo·∫°i b·ªè d·∫•u c√¢u
- **rm_numbers**: Lo·∫°i b·ªè s·ªë
- **rm_emoji**: Lo·∫°i b·ªè emoji
- **use_slang_normalization**: √Åp d·ª•ng chu·∫©n h√≥a slang/teencode theo ng√¥n ng·ªØ
- **rm_stopwords**: Lo·∫°i b·ªè stopwords theo ng√¥n ng·ªØ
- **keep_placeholders**: Gi·ªØ <URL>, <EMAIL> thay v√¨ x√≥a ho√†n to√†n
- **use_tokenize**: √Åp d·ª•ng tokenization ph√π h·ª£p v·ªõi ng√¥n ng·ªØ
- **language**: Ng√¥n ng·ªØ d·ªØ li·ªáu ("en", "vi", "auto")

**Pipeline x·ª≠ l√Ω (language-aware):**

```python
# 1. Tokenization theo ng√¥n ng·ªØ
if use_tokenize:
    text = simple_tokenize(text, language)

# 2. X·ª≠ l√Ω placeholders
if keep_placeholders:
    text = URL_PATTERN.sub(" <URL> ", text)
    text = EMAIL_PATTERN.sub(" <EMAIL> ", text)
    text = PHONE_PATTERN.sub(" <PHONE> ", text)
    text = MONEY_PATTERN.sub(" <MONEY> ", text)

# 3. Lo·∫°i b·ªè emoji
if rm_emoji:
    text = EMOJI_PATTERN.sub(" ", text)

# 4. Chuy·ªÉn th∆∞·ªùng
if lowercase:
    text = text.lower()

# 5. Chu·∫©n h√≥a slang/teencode theo ng√¥n ng·ªØ
if use_slang_normalization:
    text = normalize_slang(text, language)

# 6. Lo·∫°i b·ªè d·∫•u c√¢u
if rm_punct:
    text = text.translate(PUNCT_TABLE)

# 7. Lo·∫°i b·ªè s·ªë
if rm_numbers:
    text = re.sub(r"\d+", " ", text)

# 8. Lo·∫°i b·ªè stopwords theo ng√¥n ng·ªØ
if rm_stopwords:
    stopwords = get_stopwords(language)
    tokens = [t for t in text.split() if t not in stopwords]
    text = " ".join(tokens)

# 9. L√†m s·∫°ch kho·∫£ng tr·∫Øng cu·ªëi c√πng
text = re.sub(r"\s+", " ", text).strip()
return text
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Language detection**: Implement auto-detection v·ªõi `langdetect` library
2. **Stemming/Lemmatization**: Th√™m Porter Stemmer cho ti·∫øng Anh
3. **Performance**: Optimize cho datasets l·ªõn v·ªõi multiprocessing
4. **Flexibility**: Cho ph√©p user define custom stopwords v√† slang dictionaries

### 8. CLASS TEXTPREPROCESSOR

```python
class TextPreprocessor(BaseEstimator, TransformerMixin):
    def __init__(self, lowercase=True, rm_punct=True, ...):
        self.lowercase = lowercase
        self.rm_punct = rm_punct
        # ... c√°c tham s·ªë kh√°c

    def fit(self, X, y=None):
        return self

    def transform(self, X):
        return [basic_preprocess(x, 
                 lowercase=self.lowercase,
                 rm_punct=self.rm_punct,
                 # ... c√°c tham s·ªë kh√°c
                 ) for x in X]
```

**Gi·∫£i th√≠ch thi·∫øt k·∫ø:**
- **BaseEstimator**: Cung c·∫•p get_params(), set_params()
- **TransformerMixin**: Cung c·∫•p fit_transform()
- **fit()**: Kh√¥ng c·∫ßn h·ªçc g√¨ t·ª´ d·ªØ li·ªáu, ch·ªâ return self
- **transform()**: √Åp d·ª•ng preprocessing cho m·ªói text trong list

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Th√™m validation:** Ki·ªÉm tra tham s·ªë h·ª£p l·ªá
2. **Optimize**: S·ª≠ d·ª•ng multiprocessing cho dataset l·ªõn
3. **Add caching**: Cache k·∫øt qu·∫£ cho text gi·ªëng nhau

## üìù UNIT TESTS C·∫¶N VI·∫æT

### 1. Test Regex Patterns
```python
def test_url_pattern():
    text = "Visit https://google.com or www.facebook.com"
    result = URL_PATTERN.findall(text)
    assert len(result) == 2

def test_phone_pattern():
    text = "Call 0123456789 or +84123456789"
    result = PHONE_PATTERN.findall(text)
    assert len(result) == 2
```

### 2. Test Teencode Normalization
```python
def test_teencode_normalization():
    text = "ko c√≥ j, mik ƒëi r"
    result = normalize_teencode(text)
    expected = "kh√¥ng c√≥ g√¨, m√¨nh ƒëi r·ªìi"
    assert result == expected
```

### 3. Test Full Pipeline
```python
def test_basic_preprocess():
    text = "Ko c√≥ j!!! Visit www.google.com üòÄ"
    result = basic_preprocess(text)
    # Test k·∫øt qu·∫£ mong ƒë·ª£i
```

## üìã CHECKLIST HO√ÄN TH√ÄNH

- [ ] **Language Support**: H·ªó tr·ª£ c·∫£ ti·∫øng Anh v√† ti·∫øng Vi·ªát
- [ ] **English Stopwords**: T√≠ch h·ª£p NLTK stopwords cho ti·∫øng Anh (400+ t·ª´)
- [ ] **English Slang**: M·ªü r·ªông EN_SLANG_MAP (200+ abbreviations)
- [ ] **Regex Patterns**: T·ªëi ∆∞u cho URLs, emails, phones (international formats)
- [ ] **Language Detection**: Auto-detect v·ªõi `langdetect` library
- [ ] **Advanced Tokenization**: NLTK integration cho ti·∫øng Anh
- [ ] **Stemming/Lemmatization**: Porter Stemmer cho English texts
- [ ] **Performance**: Multiprocessing cho large datasets
- [ ] **Unit Tests**: Comprehensive tests cho c·∫£ EN/VI
- [ ] **Flexibility**: Custom dictionaries support
- [ ] **Documentation**: Detailed code comments v√† usage examples

### üåü **BONUS TASKS (N·∫øu c√≥ th·ªùi gian)**
- [ ] **spaCy Integration**: Advanced NLP preprocessing
- [ ] **Custom Models**: Train custom tokenizer cho domain-specific text
- [ ] **Caching**: Cache processed results
- [ ] **Config Files**: JSON/YAML config cho easy customization

---

# üñ•Ô∏è PH·∫¶N 2: STREAMLIT GUI & MACHINE LEARNING (Nguy·ªÖn Huy Ho√†ng)

## üéØ NHI·ªÜM V·ª§ CH√çNH
- Ph√°t tri·ªÉn giao di·ªán Streamlit ho√†n ch·ªânh
- T√≠ch h·ª£p nhi·ªÅu thu·∫≠t to√°n Machine Learning
- X√¢y d·ª±ng pipeline hu·∫•n luy·ªán v√† ƒë√°nh gi√°
- T·∫°o visualization v√† EDA
- Tri·ªÉn khai t√≠nh nƒÉng xu·∫•t m√¥ h√¨nh

## üìÅ FILE CH√çNH: `BaiTapLon.py`

### 1. THI·∫æT L·∫¨P STREAMLIT V√Ä CSS

```python
import streamlit as st
st.set_page_config(page_title="Ph√¢n lo·∫°i tin nh·∫Øn r√°c", page_icon="üì©", layout="wide")
```

**Gi·∫£i th√≠ch:**
- **page_title**: Ti√™u ƒë·ªÅ tab tr√¨nh duy·ªát
- **page_icon**: Icon hi·ªÉn th·ªã tr√™n tab
- **layout="wide"**: S·ª≠ d·ª•ng to√†n b·ªô chi·ªÅu r·ªông m√†n h√¨nh

### 2. CSS STYLING

```python
st.markdown("""
<style>
/* Import Google Fonts */
@import url('https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap');

/* Main container */
.main .block-container {
    padding-top: 2rem;
    padding-bottom: 2rem;
    max-width: 1200px;
}

/* Metric cards */
.metric-card {
    background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
    border-radius: 20px;
    padding: 25px;
    box-shadow: 0 10px 30px rgba(102, 126, 234, 0.3);
    color: white;
    text-align: center;
    transition: transform 0.3s ease;
}
</style>
""", unsafe_allow_html=True)
```

**Gi·∫£i th√≠ch CSS:**
- **Google Fonts**: Import font Inter cho UI hi·ªán ƒë·∫°i
- **Gradient backgrounds**: T·∫°o hi·ªáu ·ª©ng m√†u ƒë·∫πp
- **Box shadows**: T·∫°o ƒë·ªô s√¢u 3D
- **Transitions**: Hi·ªáu ·ª©ng hover m∆∞·ª£t m√†

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **T√πy ch·ªânh theme**: T·∫°o color scheme nh·∫•t qu√°n
2. **Responsive design**: ƒê·∫£m b·∫£o hi·ªÉn th·ªã t·ªët tr√™n mobile
3. **Dark mode**: Th√™m toggle chuy·ªÉn ƒë·ªïi theme

### 3. SIDEBAR C·∫§U H√åNH

```python
st.sidebar.title("‚öôÔ∏è C·∫•u h√¨nh th√≠ nghi·ªám")

# 1. Ngu·ªìn d·ªØ li·ªáu
data_source = st.sidebar.radio("Ch·ªçn ngu·ªìn d·ªØ li·ªáu", 
    ["üìÅ Upload CSV", "üìä D·ªØ li·ªáu m·∫´u", "üåê Kaggle Dataset"], 
    index=1)

# 2. Ti·ªÅn x·ª≠ l√Ω
lowercase = st.sidebar.checkbox("Lowercase", value=True)
use_tokenize = st.sidebar.checkbox("T√°ch t·ª´ ti·∫øng Vi·ªát", value=True)
rm_punct = st.sidebar.checkbox("B·ªè d·∫•u c√¢u", value=True)
```

**Gi·∫£i th√≠ch components:**
- **radio**: Ch·ªçn m·ªôt trong nhi·ªÅu options
- **checkbox**: Toggle boolean options
- **slider**: Ch·ªçn gi√° tr·ªã trong kho·∫£ng
- **selectbox**: Dropdown menu

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Th√™m tooltips**: Gi·∫£i th√≠ch cho t·ª´ng option
2. **Grouping**: Nh√≥m c√°c c·∫•u h√¨nh li√™n quan
3. **Validation**: Ki·ªÉm tra t√≠nh h·ª£p l·ªá c·ªßa input

### 4. X·ª¨ L√ù UPLOAD FILE

```python
if data_source == "üìÅ Upload CSV":
    uploaded_file = st.sidebar.file_uploader("Ch·ªçn file CSV", type=['csv'])
    if uploaded_file is not None:
        try:
            df_raw = pd.read_csv(uploaded_file, encoding="utf-8")
        except Exception:
            uploaded_file.seek(0)
            df_raw = pd.read_csv(uploaded_file, encoding="latin-1")
```

**Gi·∫£i th√≠ch x·ª≠ l√Ω:**
1. **file_uploader**: Widget upload file
2. **Multiple encodings**: Th·ª≠ UTF-8 tr∆∞·ªõc, fallback v·ªÅ latin-1
3. **seek(0)**: Reset file pointer v·ªÅ ƒë·∫ßu

**Auto-detection columns:**
```python
# Nh·∫≠n di·ªán c·ªôt text/label
lower_map = {c.lower(): c for c in df_raw.columns}
label_candidates = ["label", "category", "class", "target", "v1"]
text_candidates = ["text", "message", "sms", "content", "body", "v2"]
label_col = next((lower_map[k] for k in label_candidates if k in lower_map), None)
text_col = next((lower_map[k] for k in text_candidates if k in lower_map), None)
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **M·ªü r·ªông detection**: Th√™m nhi·ªÅu t√™n c·ªôt ph·ªï bi·∫øn
2. **Preview data**: Hi·ªÉn th·ªã sample data tr∆∞·ªõc khi x·ª≠ l√Ω
3. **Data validation**: Ki·ªÉm tra format, missing values

### 5. EXPLORATORY DATA ANALYSIS (EDA)

```python
# Data stats cards
col1, col2, col3, col4 = st.columns(4)

with col1:
    st.markdown(
        f'<div class="metric-card"><div class="subtle">T·ªïng s·ªë d√≤ng</div><div class="kpi">{len(df)}</div></div>',
        unsafe_allow_html=True
    )
```

**Visualization v·ªõi Plotly:**
```python
# Pie chart ph√¢n ph·ªëi nh√£n
label_counts = df['label'].value_counts()
fig = px.pie(values=label_counts.values, names=label_counts.index, 
            title="Ph√¢n ph·ªëi nh√£n")
st.plotly_chart(fig, use_container_width=True)

# Histogram ƒë·ªô d√†i vƒÉn b·∫£n
text_lengths = df['text'].str.len()
fig = px.histogram(x=text_lengths, nbins=30, title="Ph√¢n ph·ªëi ƒë·ªô d√†i vƒÉn b·∫£n")
st.plotly_chart(fig, use_container_width=True)
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Word clouds**: T·∫°o word cloud cho spam vs ham
2. **Statistical tests**: Chi-square test cho independence
3. **Advanced plots**: Violin plots, heatmaps
4. **Interactive filters**: Filter data theo criteria

### 6. MACHINE LEARNING PIPELINE

```python
# Vectorizers
if feat_type == "BoW":
    vectorizer = CountVectorizer(ngram_range=(1, ngram_max), min_df=min_df, max_df=max_df/100.0)
elif feat_type == "TF-IDF":
    vectorizer = TfidfVectorizer(ngram_range=(1, ngram_max), min_df=min_df, max_df=max_df/100.0)

# Models
if model_name == "Naive Bayes (Multinomial)":
    clf = MultinomialNB(alpha=alpha)
elif model_name == "Logistic Regression":
    clf = LogisticRegression(C=C, max_iter=max_iter)
elif model_name == "Linear SVM":
    clf = LinearSVC(C=C)
```

**Pipeline construction:**
```python
pipe = Pipeline([
    ("prep", TextPreprocessor(lowercase=lowercase, rm_punct=rm_punct, ...)),
    ("vec", vectorizer),
    ("clf", clf)
])
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Th√™m models**: XGBoost, LightGBM, Neural Networks
2. **Hyperparameter tuning**: GridSearchCV, RandomizedSearchCV
3. **Cross-validation**: K-fold v·ªõi stratification
4. **Feature selection**: Chi2, mutual info, ANOVA

### 7. TRAINING V√Ä EVALUATION

```python
# Train/test split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=test_size, random_state=random_state, stratify=y
)

# Training
start = time.time()
pipe.fit(X_train, y_train)
train_time = time.time() - start

# Prediction v√† metrics
y_pred = pipe.predict(X_test)
acc = accuracy_score(y_test, y_pred)
pr, rc, f1, _ = precision_recall_fscore_support(y_test, y_pred, average="binary")
```

**Confusion Matrix visualization:**
```python
cm = confusion_matrix(y_test, y_pred)
fig, ax = plt.subplots(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)
ax.set_title('Confusion Matrix')
st.pyplot(fig)
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **ROC Curves**: Plot ROC cho multiple models
2. **Learning curves**: Validation curves
3. **Error analysis**: Analyze false positives/negatives
4. **Model comparison**: Side-by-side comparison table

### 8. MODEL EXPORT V√Ä DEPLOYMENT

```python
# Export PKL model
buffer = io.BytesIO()
joblib.dump(pipe, buffer)
buffer.seek(0)
st.download_button(
    label="üì• Model (PKL)",
    data=buffer,
    file_name="spam_classifier_pipeline.pkl",
    mime="application/octet-stream"
)

# Export metadata CSV
model_metadata = {
    "metric": ["accuracy", "precision", "recall", "f1_score"],
    "value": [acc, pr, rc, f1],
    "model_type": [model_name] * 4
}
metadata_df = pd.DataFrame(model_metadata)
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **ONNX export**: Convert model to ONNX format
2. **Model versioning**: Version control cho models
3. **A/B testing**: Compare model versions
4. **Performance monitoring**: Track model drift

### 9. REAL-TIME PREDICTION

```python
predict_text = st.text_area("Nh·∫≠p tin nh·∫Øn mu·ªën ki·ªÉm tra", height=100)
if st.button("üîç Ph√¢n lo·∫°i"):
    if st.session_state.trained_model is not None:
        pred = st.session_state.trained_model.predict([predict_text])[0]
        
        # Display prediction v·ªõi style ƒë·∫πp
        if pred.lower() == 'spam':
            st.markdown("""
            <div style="background: linear-gradient(135deg, #ff6b6b 0%, #ee5a24 100%); 
                        padding: 2rem; border-radius: 20px; text-align: center; color: white;">
                <h3>üö® SPAM</h3>
            </div>
            """, unsafe_allow_html=True)
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Batch prediction**: Upload file ƒë·ªÉ predict nhi·ªÅu tin nh·∫Øn
2. **Confidence scores**: Hi·ªÉn th·ªã ƒë·ªô tin c·∫≠y
3. **Explanation**: LIME/SHAP ƒë·ªÉ gi·∫£i th√≠ch prediction
4. **History**: L∆∞u l·∫°i prediction history

## üìã CHECKLIST HO√ÄN TH√ÄNH

- [ ] Giao di·ªán Streamlit ƒë·∫πp v·ªõi CSS custom
- [ ] EDA section ho√†n ch·ªânh v·ªõi charts
- [ ] 7+ thu·∫≠t to√°n ML v·ªõi hyperparameter tuning
- [ ] Train/validation/test pipeline
- [ ] Confusion matrix v√† classification report
- [ ] Model comparison dashboard
- [ ] Export functionality (PKL, CSV, metadata)
- [ ] Real-time prediction interface
- [ ] Error handling v√† user experience
- [ ] Documentation v√† help tooltips

---

# üîó PH·∫¶N 3: REST API & DEPLOYMENT (Nguy·ªÖn Minh S∆°n)

## üéØ NHI·ªÜM V·ª§ CH√çNH
- Ph√°t tri·ªÉn FastAPI REST API
- T√≠ch h·ª£p model loading v√† prediction
- Tri·ªÉn khai logging v√† error handling
- T·∫°o Docker container
- Vi·∫øt documentation v√† deployment guides

## üìÅ FILE CH√çNH: `api.py`

### 1. THI·∫æT L·∫¨P FASTAPI

```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field

app = FastAPI(
    title="Spam Classifier API",
    description="API ph√¢n lo·∫°i tin nh·∫Øn r√°c s·ª≠ d·ª•ng Machine Learning",
    version="1.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)
```

**Gi·∫£i th√≠ch metadata:**
- **title**: T√™n API hi·ªÉn th·ªã trong docs
- **description**: M√¥ t·∫£ ch·ª©c nƒÉng
- **version**: Version API cho tracking
- **docs_url**: Swagger UI endpoint
- **redoc_url**: ReDoc documentation endpoint

### 2. CORS MIDDLEWARE

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Trong production n√™n ch·ªâ ƒë·ªãnh c·ª• th·ªÉ
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

**Gi·∫£i th√≠ch CORS:**
- **allow_origins**: Domain ƒë∆∞·ª£c ph√©p truy c·∫≠p API
- **allow_credentials**: Cho ph√©p g·ª≠i cookies/auth headers
- **allow_methods**: HTTP methods ƒë∆∞·ª£c ph√©p
- **allow_headers**: Headers ƒë∆∞·ª£c ph√©p

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Security**: Restrict origins trong production
2. **Rate limiting**: Implement rate limiting middleware
3. **Authentication**: JWT authentication n·∫øu c·∫ßn

### 3. MODEL LOADING

```python
MODEL_PATH = "spam_classifier_pipeline.pkl"
model = None

def load_model():
    global model
    try:
        if os.path.exists(MODEL_PATH):
            model = joblib.load(MODEL_PATH)
            logger.info(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh th√†nh c√¥ng t·ª´ {MODEL_PATH}")
            return True
        else:
            logger.error(f"‚ùå Kh√¥ng t√¨m th·∫•y file m√¥ h√¨nh: {MODEL_PATH}")
            return False
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh: {str(e)}")
        return False

# T·∫£i m√¥ h√¨nh khi kh·ªüi ƒë·ªông
load_model()
```

**Gi·∫£i th√≠ch pattern:**
- **Global variable**: L∆∞u model trong memory
- **Startup loading**: T·∫£i model khi API kh·ªüi ƒë·ªông
- **Error handling**: Graceful handling khi model kh√¥ng t·∫£i ƒë∆∞·ª£c
- **Logging**: Track loading status

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Model caching**: Redis cache cho multiple models
2. **Hot reload**: Reload model kh√¥ng restart server
3. **Model versioning**: Support multiple model versions
4. **Health monitoring**: Monitor model performance

### 4. PYDANTIC MODELS

```python
class PredictRequest(BaseModel):
    text: str = Field(..., description="Tin nh·∫Øn c·∫ßn ph√¢n lo·∫°i", min_length=1, max_length=1000)

class PredictResponse(BaseModel):
    text: str
    prediction: str
    confidence: Optional[float] = None
    status: str

class HealthResponse(BaseModel):
    status: str
    model_loaded: bool
    message: str
```

**Gi·∫£i th√≠ch Pydantic:**
- **BaseModel**: Base class cho data validation
- **Field**: Define constraints v√† metadata
- **Optional**: Field c√≥ th·ªÉ None
- **Type hints**: Python type annotations

**Validation benefits:**
- **Automatic validation**: FastAPI t·ª± validate input
- **Error responses**: Tr·∫£ v·ªÅ error messages chi ti·∫øt
- **Documentation**: Auto-generate API docs
- **IDE support**: Type hints cho autocomplete

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Advanced validation**: Custom validators
2. **Response models**: Standardize error responses
3. **Nested models**: Complex data structures
4. **Examples**: Add example values trong docs

### 5. HEALTH CHECK ENDPOINT

```python
@app.get("/health", response_model=HealthResponse, tags=["Health"])
def health_check():
    """
    Ki·ªÉm tra tr·∫°ng th√°i ho·∫°t ƒë·ªông c·ªßa API v√† m√¥ h√¨nh
    """
    return HealthResponse(
        status="healthy" if model is not None else "unhealthy",
        model_loaded=model is not None,
        message="API ƒëang ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng" if model is not None else "M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i"
    )
```

**Gi·∫£i th√≠ch decorator:**
- **@app.get**: HTTP GET endpoint
- **response_model**: Pydantic model cho response
- **tags**: Nh√≥m endpoints trong docs

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Extended health**: Check database, external services
2. **Metrics**: Response time, memory usage
3. **Dependencies**: Check all service dependencies
4. **Alerting**: Integration v·ªõi monitoring systems

### 6. PREDICTION ENDPOINT

```python
@app.post("/predict", response_model=PredictResponse, tags=["Prediction"])
def predict(request: PredictRequest):
    if model is None:
        raise HTTPException(
            status_code=503, 
            detail="M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng ki·ªÉm tra l·∫°i."
        )

    try:
        # D·ª± ƒëo√°n
        prediction = model.predict([request.text])[0]
        
        # T√≠nh confidence n·∫øu c√≥ th·ªÉ
        confidence = None
        if hasattr(model.named_steps["clf"], "predict_proba"):
            probabilities = model.predict_proba([request.text])[0]
            classes = model.named_steps["clf"].classes_
            pred_idx = list(classes).index(prediction)
            confidence = float(probabilities[pred_idx])
        
        return PredictResponse(
            text=request.text,
            prediction=prediction,
            confidence=confidence,
            status="success"
        )
        
    except Exception as e:
        logger.error(f"L·ªói khi d·ª± ƒëo√°n: {str(e)}")
        raise HTTPException(status_code=500, detail=f"L·ªói trong qu√° tr√¨nh d·ª± ƒëo√°n: {str(e)}")
```

**Gi·∫£i th√≠ch x·ª≠ l√Ω:**
1. **Model check**: Ki·ªÉm tra model ƒë√£ load ch∆∞a
2. **Prediction**: G·ªçi model.predict()
3. **Confidence calculation**: T√≠nh probability n·∫øu c√≥
4. **Error handling**: Catch v√† return HTTP errors

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Batch prediction**: Endpoint cho multiple texts
2. **Async processing**: Background tasks cho prediction l√¢u
3. **Caching**: Cache prediction results
4. **Input sanitization**: Clean v√† validate input text

### 7. LOGGING SYSTEM

```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Trong functions
logger.info(f"‚úÖ ƒê√£ t·∫£i m√¥ h√¨nh th√†nh c√¥ng t·ª´ {MODEL_PATH}")
logger.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh: {str(e)}")
logger.warning(f"Kh√¥ng th·ªÉ t√≠nh confidence: {str(e)}")
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Structured logging**: JSON format logs
2. **Log rotation**: Rotate log files
3. **Centralized logging**: ELK stack integration
4. **Performance logging**: Request/response times

### 8. ERROR HANDLING

```python
from fastapi import HTTPException

# Service unavailable
raise HTTPException(status_code=503, detail="M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i")

# Internal server error
raise HTTPException(status_code=500, detail=f"L·ªói: {str(e)}")

# Bad request
raise HTTPException(status_code=400, detail="Input kh√¥ng h·ª£p l·ªá")
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Custom exception handlers**: Global error handling
2. **Error codes**: Standardized error codes
3. **User-friendly messages**: Vietnamese error messages
4. **Monitoring**: Error tracking v√† alerting

### 9. MODEL INFO ENDPOINT

```python
@app.get("/model-info", tags=["Info"])
def get_model_info():
    if model is None:
        raise HTTPException(status_code=503, detail="M√¥ h√¨nh ch∆∞a ƒë∆∞·ª£c t·∫£i")
    
    try:
        info = {
            "model_type": type(model).__name__,
            "steps": list(model.named_steps.keys()) if hasattr(model, 'named_steps') else [],
            "classes": model.named_steps["clf"].classes_.tolist() if "clf" in model.named_steps else None
        }
        return info
    except Exception as e:
        raise HTTPException(status_code=500, detail="Kh√¥ng th·ªÉ l·∫•y th√¥ng tin m√¥ h√¨nh")
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Extended info**: Feature names, model params
2. **Performance metrics**: Accuracy, training time
3. **Version info**: Model version, training date
4. **Data info**: Training data statistics

## üê≥ DOCKER DEPLOYMENT

### 1. DOCKERFILE

```dockerfile
FROM python:3.9-slim

# Set working directory
WORKDIR /app

# Install system dependencies
RUN apt-get update && apt-get install -y \
    gcc \
    && rm -rf /var/lib/apt/lists/*

# Copy requirements first for better caching
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application code
COPY . .

# Expose port
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Run application
CMD ["uvicorn", "api:app", "--host", "0.0.0.0", "--port", "8000"]
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Multi-stage build**: Optimize image size
2. **Security**: Non-root user, minimal dependencies
3. **Performance**: Gunicorn v·ªõi multiple workers
4. **Monitoring**: Add monitoring tools

### 2. DOCKER-COMPOSE

```yaml
version: '3.8'

services:
  spam-classifier-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - LOG_LEVEL=info
    volumes:
      - ./models:/app/models
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - spam-classifier-api
    restart: unless-stopped
```

**üîß NHI·ªÜM V·ª§ C·ª¶A B·∫†N:**
1. **Load balancer**: Multiple API instances
2. **SSL termination**: HTTPS setup
3. **Monitoring**: Prometheus, Grafana
4. **Database**: If needed for logging/analytics

### 3. KUBERNETES DEPLOYMENT

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: spam-classifier-api
spec:
  replicas: 3
  selector:
    matchLabels:
      app: spam-classifier-api
  template:
    metadata:
      labels:
        app: spam-classifier-api
    spec:
      containers:
      - name: api
        image: spam-classifier-api:latest
        ports:
        - containerPort: 8000
        resources:
          requests:
            memory: "512Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
```

## üìö API DOCUMENTATION

### 1. README.md cho API

```markdown
# Spam Classifier API

## Endpoints

### POST /predict
Ph√¢n lo·∫°i tin nh·∫Øn

**Request:**
```json
{
  "text": "Khuy·∫øn m√£i 50% t·∫•t c·∫£ s·∫£n ph·∫©m!"
}
```

**Response:**
```json
{
  "text": "Khuy·∫øn m√£i 50% t·∫•t c·∫£ s·∫£n ph·∫©m!",
  "prediction": "spam",
  "confidence": 0.95,
  "status": "success"
}
```

### GET /health
Ki·ªÉm tra tr·∫°ng th√°i API

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "message": "API ƒëang ho·∫°t ƒë·ªông b√¨nh th∆∞·ªùng"
}
```
```

### 2. POSTMAN COLLECTION

```json
{
  "info": {
    "name": "Spam Classifier API",
    "description": "Collection for testing spam classifier API"
  },
  "item": [
    {
      "name": "Health Check",
      "request": {
        "method": "GET",
        "header": [],
        "url": {
          "raw": "{{base_url}}/health",
          "host": ["{{base_url}}"],
          "path": ["health"]
        }
      }
    },
    {
      "name": "Predict Spam",
      "request": {
        "method": "POST",
        "header": [
          {
            "key": "Content-Type",
            "value": "application/json"
          }
        ],
        "body": {
          "mode": "raw",
          "raw": "{\n  \"text\": \"Khuy·∫øn m√£i 50% t·∫•t c·∫£ s·∫£n ph·∫©m!\"\n}"
        },
        "url": {
          "raw": "{{base_url}}/predict",
          "host": ["{{base_url}}"],
          "path": ["predict"]
        }
      }
    }
  ]
}
```

## üìã CHECKLIST HO√ÄN TH√ÄNH

- [ ] FastAPI v·ªõi 5+ endpoints ho√†n ch·ªânh
- [ ] Pydantic models v·ªõi validation
- [ ] Error handling v√† logging system
- [ ] Model loading v√† caching
- [ ] Health checks v√† monitoring
- [ ] CORS v√† security middleware
- [ ] Docker containerization
- [ ] Kubernetes deployment manifests
- [ ] API documentation (Swagger + README)
- [ ] Postman collection
- [ ] Performance testing scripts
- [ ] Production deployment guide

---

# ü§ù T√çCH H·ª¢P V√Ä PH·ªêI H·ª¢P

## üìÖ TIMELINE T√çCH H·ª¢P

### Tu·∫ßn 1: Development song song
- **Ng∆∞·ªùi 1**: Ho√†n th√†nh `text_preprocessor.py`
- **Ng∆∞·ªùi 2**: X√¢y d·ª±ng Streamlit UI c∆° b·∫£n
- **Ng∆∞·ªùi 3**: Setup FastAPI endpoints

### Tu·∫ßn 2: Integration Points
- **Ng∆∞·ªùi 1 ‚Üí 2**: Merge text preprocessor v√†o Streamlit
- **Ng∆∞·ªùi 2**: Ho√†n thi·ªán ML pipeline v·ªõi preprocessing
- **Ng∆∞·ªùi 3**: Test API v·ªõi sample models

### Tu·∫ßn 3: Final Integration
- **Ng∆∞·ªùi 2 ‚Üí 3**: Export trained models cho API
- **Ng∆∞·ªùi 3**: T√≠ch h·ª£p v√† test to√†n b·ªô system
- **All**: Testing, documentation, deployment

## üîó INTEGRATION CHECKLIST

- [ ] `text_preprocessor.py` t∆∞∆°ng th√≠ch v·ªõi c·∫£ Streamlit v√† API
- [ ] Model export t·ª´ Streamlit ho·∫°t ƒë·ªông v·ªõi API
- [ ] Consistent input/output formats
- [ ] Error handling nh·∫•t qu√°n
- [ ] Logging formats t∆∞∆°ng th√≠ch
- [ ] Documentation ƒë·∫ßy ƒë·ªß cho c·∫£ 3 ph·∫ßn

## üìû COMMUNICATION PROTOCOL

1. **Daily standup**: 15 ph√∫t m·ªói ng√†y
2. **Git workflow**: Feature branches + Pull Requests
3. **Code review**: Peer review tr∆∞·ªõc khi merge
4. **Testing**: Unit tests + Integration tests
5. **Documentation**: Code comments + README updates

---

**üéØ CH√öC C√ÅC B·∫†N TH·ª∞C HI·ªÜN TH√ÄNH C√îNG!**
